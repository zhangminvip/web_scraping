这是一个自动爬取网页和存储想要的数据的源代码，我们可以读取网站的robots文件，看是否我们的爬虫活动是否在网站的允许范围内，在link_crawler中，我们可以设置，爬虫的初始网址（种子网址，），爬取的url符合的规则，以及爬取过程中的延迟，最大深度，最多爬取的网页数，爬虫的header,user_agent,
proxy,失败重新爬取的的尝试次数，以及一个回调函数，我们运行应该从scrape_callback中运行。